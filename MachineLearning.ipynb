{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "\n",
    "from sklearn import metrics\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "#sfrom sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printTree(feature_cols,tree):\n",
    "    dot_data = StringIO()\n",
    "    export_graphviz(tree, out_file=dot_data,  \n",
    "                    filled=True, rounded=True,\n",
    "                    special_characters=True, feature_names = feature_cols,class_names=['0','1'])\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "    graph.write_png('magic_tree.png')\n",
    "    Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    #classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcMetrics(Yt, Yp, classes, model, norm=True, cols=None, print_metrics=False):\n",
    "    # Metrics\n",
    "    acc = metrics.accuracy_score(Yt, Yp)\n",
    "    acc_bal = metrics.balanced_accuracy_score(Yt, Yp)\n",
    "    prec = metrics.precision_score(Yt, Yp)\n",
    "    recall = metrics.recall_score(Yt, Yp)\n",
    "    f1 = metrics.f1_score(Yt, Yp)\n",
    "    roc_auc = roc_auc_score(Yt, Yp)\n",
    "    \n",
    "    feature_imp = pd.Series(model.feature_importances_,index=cols).sort_values(ascending=True)\n",
    "    \n",
    "    if(print_metrics):\n",
    "        print(\"\")\n",
    "        plot_confusion_matrix(Yt, Yp, classes, normalize=norm)\n",
    "        print(\"\")\n",
    "        print(\"Accuracy:\", acc)\n",
    "        print(\"Balanced Accuracy: \", acc_bal)\n",
    "        print(\"Precision: \", prec)\n",
    "        print(\"Recall: \", recall)\n",
    "        print(\"F1 Score: \", f1)\n",
    "        print(\"ROC AuC Score: \", roc_auc)\n",
    "       \n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "        ax.barh(feature_imp.index,feature_imp.values, align='center')\n",
    "        ax.set_xlabel('Performance')\n",
    "        ax.set_title('How fast do you want to go today?')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    return acc, acc_bal, prec, recall, f1, roc_auc, feature_imp\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainDecisionTree(Xtr, Xte, Ytr, Yte, feat_cols, print_metrics=False):\n",
    "    tree = DecisionTreeClassifier()\n",
    "    tree = tree.fit(Xtr,Ytr)\n",
    "    y_pred = tree.predict(Xte)\n",
    "    \n",
    "    return calcMetrics(Yte, y_pred, [0,1], tree, norm=True, cols=feat_cols, print_metrics=print_metrics)\n",
    "\n",
    "\n",
    "def trainRandomForest(Xtr, Xte, Ytr, Yte, feat_cols, print_metrics=False):\n",
    "    forest=RandomForestClassifier(n_estimators=1000)\n",
    "    forest.fit(Xtr,Ytr)\n",
    "    y_pred=forest.predict(Xte)\n",
    "    \n",
    "    return calcMetrics(Yte, y_pred, [0,1], forest, norm=True, cols=feat_cols, print_metrics=print_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainGradientBoosting(Xtr, Xte, Ytr, Yte, feat_cols, print_metrics=False):\n",
    "    model_GB = GradientBoostingClassifier(n_estimators=1000)\n",
    "    model_GB.fit(Xtr , Ytr)\n",
    "    y_pred = model_GB.predict(Xte)\n",
    "    \n",
    "    return calcMetrics(Yte, y_pred, [0,1], model_GB, norm=True,\n",
    "                       cols=feat_cols, print_metrics=print_metrics)\n",
    "\n",
    "    #print(classification_report(Y_test, y_pred,target_names=target_names))\n",
    "    \n",
    "def trainADA(Xtr, Xte, Ytr, Yte, feat_cols, print_metrics=False):\n",
    "    model_ad = AdaBoostClassifier()\n",
    "    model_ad.fit(Xtr , Ytr)\n",
    "    y_pred = model_ad.predict(Xte)\n",
    "    \n",
    "    return calcMetrics(Yte, y_pred, [0,1], model_ad, norm=True,\n",
    "                       cols=feat_cols, print_metrics=print_metrics)\n",
    "\n",
    "    #print(classification_report(Y_test, y_pred,target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_files = [\n",
    "    \"features/features_sub.csv\",\n",
    "    \"features/features_div.csv\"\n",
    "]\n",
    "\n",
    "feature_cols = [\n",
    "        'duration',\n",
    "        #'show_order',\n",
    "        'fix_freq','sacc_freq',\n",
    "        'pupil_diam_right_mean','pupil_diam_right_std','pupil_diam_right_min','pupil_diam_right_max',\n",
    "        'pupil_diam_left_mean','pupil_diam_left_std','pupil_diam_left_min','pupil_diam_left_max',\n",
    "        'sre_fix_freq','sre_sacc_freq',\n",
    "        'sre_pupil_diam_right_mean','sre_pupil_diam_right_std',\n",
    "        'sre_pupil_diam_right_min','sre_pupil_diam_right_max',\n",
    "        'sre_pupil_diam_left_mean','sre_pupil_diam_left_std',\n",
    "        'sre_pupil_diam_left_min','sre_pupil_diam_left_max',\n",
    "        'srl_fix_freq','srl_sacc_freq',\n",
    "        'srl_pupil_diam_right_mean','srl_pupil_diam_right_std',\n",
    "        'srl_pupil_diam_right_min','srl_pupil_diam_right_max',\n",
    "        'srl_pupil_diam_left_mean','srl_pupil_diam_left_std',\n",
    "        'srl_pupil_diam_left_min','srl_pupil_diam_left_max',\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelMetricAggregator:\n",
    "    \n",
    "    def __init__(self, model, mode=''):\n",
    "        self.model = model\n",
    "        self.mode = mode\n",
    "        self.accuracy = []\n",
    "        self.balanced_accuracy = []\n",
    "        self.precision = []\n",
    "        self.recall = []\n",
    "        self.f1_score = []\n",
    "        self.roc_auc = []\n",
    "        \n",
    "    def addMetrics(self,acc, bal_acc, prec, rec, f1, roc_auc):\n",
    "        self.accuracy.append(acc)\n",
    "        self.balanced_accuracy.append(bal_acc)\n",
    "        self.precision.append(prec)\n",
    "        self.recall.append(rec)\n",
    "        self.f1_score.append(f1)\n",
    "        self.roc_auc = roc_auc\n",
    "        \n",
    "    def getMetrics(self):\n",
    "        return [\n",
    "            self.model,\n",
    "            self.mode,\n",
    "            np.mean(self.accuracy),\n",
    "            np.mean(self.balanced_accuracy),\n",
    "            np.mean(self.precision),\n",
    "            np.mean(self.recall),\n",
    "            np.mean(self.f1_score),\n",
    "            np.mean(self.roc_auc)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process File: features/features_sub.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dario/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/dario/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/dario/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/dario/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/dario/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/dario/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/dario/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/dario/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/dario/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/dario/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/dario/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/dario/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/dario/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/dario/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/dario/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/dario/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/dario/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/dario/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process File: features/features_div.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dario/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/dario/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/dario/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/dario/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/dario/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/dario/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/dario/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/dario/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/dario/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/dario/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/dario/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/dario/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy        1\n",
      "bal_accuracy    7\n",
      "precision       7\n",
      "recall          7\n",
      "f1_score        7\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>baseline</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>bal_accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tree</td>\n",
       "      <td>sub</td>\n",
       "      <td>0.655455</td>\n",
       "      <td>0.496111</td>\n",
       "      <td>0.115000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.145238</td>\n",
       "      <td>0.4375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>forest</td>\n",
       "      <td>sub</td>\n",
       "      <td>0.742727</td>\n",
       "      <td>0.510417</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.068571</td>\n",
       "      <td>0.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gb</td>\n",
       "      <td>sub</td>\n",
       "      <td>0.714545</td>\n",
       "      <td>0.511528</td>\n",
       "      <td>0.116667</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.3125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ada</td>\n",
       "      <td>sub</td>\n",
       "      <td>0.635455</td>\n",
       "      <td>0.482500</td>\n",
       "      <td>0.098333</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.140476</td>\n",
       "      <td>0.3750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tree</td>\n",
       "      <td>div</td>\n",
       "      <td>0.550909</td>\n",
       "      <td>0.396389</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.3750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>forest</td>\n",
       "      <td>div</td>\n",
       "      <td>0.713636</td>\n",
       "      <td>0.493611</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.068571</td>\n",
       "      <td>0.3750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gb</td>\n",
       "      <td>div</td>\n",
       "      <td>0.686364</td>\n",
       "      <td>0.477222</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.3125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ada</td>\n",
       "      <td>div</td>\n",
       "      <td>0.687273</td>\n",
       "      <td>0.540278</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.185238</td>\n",
       "      <td>0.4375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  baseline model  accuracy  bal_accuracy  precision  recall  f1_score  roc_auc\n",
       "0     tree   sub  0.655455      0.496111   0.115000    0.25  0.145238   0.4375\n",
       "1   forest   sub  0.742727      0.510417   0.050000    0.15  0.068571   0.5000\n",
       "2       gb   sub  0.714545      0.511528   0.116667    0.20  0.140000   0.3125\n",
       "3      ada   sub  0.635455      0.482500   0.098333    0.25  0.140476   0.3750\n",
       "4     tree   div  0.550909      0.396389   0.050000    0.15  0.075000   0.3750\n",
       "5   forest   div  0.713636      0.493611   0.050000    0.15  0.068571   0.3750\n",
       "6       gb   div  0.686364      0.477222   0.083333    0.15  0.100000   0.3125\n",
       "7      ada   div  0.687273      0.540278   0.150000    0.30  0.185238   0.4375"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_cols = ['baseline','model', \n",
    "                                'accuracy', 'bal_accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']\n",
    "mDF = pd.DataFrame(columns=metrics_cols)\n",
    "\n",
    "for file in feature_files:\n",
    "    print(\"Process File: {}\".format(file))\n",
    "    \n",
    "    f = file.split('/')[-1]\n",
    "    mode = file.split('/')[-1].split('.')[0].split('_')\n",
    "    \n",
    "    # Load the file\n",
    "    features = pd.read_csv(file, sep='\\t')\n",
    "    \n",
    "    if(mode[1] == \"sub\"):\n",
    "        features = features.fillna(0)\n",
    "    else:\n",
    "        features = features.fillna(1)\n",
    "    \n",
    "    # Extract Feature Columns\n",
    "    X = features[feature_cols]\n",
    "    y = features['label']\n",
    "    \n",
    "    # Rebalance the dataset with SMOTE\n",
    "    #X, y = SMOTE().fit_resample(X, y)\n",
    "    \n",
    "    # Create Folds for KFold Cross Validation\n",
    "    # 102 for leave one out\n",
    "    splitter = KFold(n_splits=10, random_state=11)\n",
    "    #splitter = LeaveOneOut()\n",
    "    \n",
    "    # Initialize Metrics accumulators\n",
    "    tree = ModelMetricAggregator(\"tree\", mode[1])\n",
    "    forest = ModelMetricAggregator(\"forest\", mode[1])\n",
    "    gboost = ModelMetricAggregator(\"gb\", mode[1])\n",
    "    ada = ModelMetricAggregator(\"ada\", mode[1])\n",
    "    \n",
    "    for train_index, test_index in splitter.split(X):\n",
    "        #print(\"train index: \", train_index)\n",
    "        \n",
    "        # Generate Train and Test set\n",
    "        X_train = X.iloc[train_index]        \n",
    "        y_train = y.iloc[train_index]\n",
    "        X_test = X.iloc[test_index]\n",
    "        y_test = y.iloc[test_index]\n",
    "        \n",
    "        # Smothe inside the fold\n",
    "        X_train, y_train = SMOTE().fit_resample(X_train, y_train)\n",
    "        \n",
    "        # Train Decision Tree\n",
    "        t_acc, t_bal, t_prec, t_rec, t_f1, t_auc, t_best_feat =  \\\n",
    "                        trainDecisionTree(X_train, X_test, y_train, y_test, feature_cols)\n",
    "        tree.addMetrics(t_acc, t_bal, t_prec, t_rec, t_f1, t_auc)\n",
    "        \n",
    "        # Train Random Forest\n",
    "        f_acc, f_bal, f_prec, f_rec, f_f1, f_auc, f_best_feat = \\\n",
    "                        trainRandomForest(X_train, X_test, y_train, y_test, feature_cols)\n",
    "        forest.addMetrics(f_acc, f_bal, f_prec, f_rec, f_f1, f_auc)\n",
    "        \n",
    "        g_acc, g_bal, g_prec, g_rec, g_f1, g_auc, g_best_feat =  \\\n",
    "                        trainGradientBoosting(X_train, X_test, y_train, y_test, feature_cols)\n",
    "        gboost.addMetrics(g_acc, g_bal, g_prec, g_rec, g_f1, g_auc)\n",
    "        \n",
    "        a_acc, a_bacc, a_prec, a_rec, a_f1, a_auc, a_best = trainADA(X_train, X_test, y_train, y_test, feature_cols)\n",
    "        ada.addMetrics(a_acc, a_bacc, a_prec, a_rec, a_f1, a_auc)\n",
    "        \n",
    "    # Calculate KFold Mean results\n",
    "    tree_row = tree.getMetrics()\n",
    "    forest_row = forest.getMetrics()\n",
    "    g_row = gboost.getMetrics()\n",
    "    ada_row = ada.getMetrics()\n",
    "    \n",
    "    tree_series = pd.Series(tree_row, index=metrics_cols)\n",
    "    forest_series = pd.Series(forest_row, index=metrics_cols)\n",
    "    g_series = pd.Series(g_row, index=metrics_cols)\n",
    "    ada_series = pd.Series(ada_row, index=metrics_cols)\n",
    "   \n",
    "    mDF = mDF.append(tree_series, ignore_index=True)\n",
    "    mDF = mDF.append(forest_series, ignore_index=True)\n",
    "    mDF = mDF.append(g_series, ignore_index=True)\n",
    "    mDF = mDF.append(ada_series, ignore_index=True)\n",
    "\n",
    "\n",
    "#mDF.to_csv(\"metrics.csv\")    \n",
    "idx = mDF[['accuracy', 'bal_accuracy', 'precision', 'recall', 'f1_score']].idxmax()\n",
    "print(idx)\n",
    "\n",
    "mDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 \ttree \tsub \t0.637019 \t0.524242 \t0.199107 \t0.354167 \t0.245040 \t0.35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
