{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "\n",
    "from sklearn import metrics\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "#sfrom sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "\n",
    "# Keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import metrics as km\n",
    "from keras import losses\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printTree(feature_cols,tree):\n",
    "    dot_data = StringIO()\n",
    "    export_graphviz(tree, out_file=dot_data,  \n",
    "                    filled=True, rounded=True,\n",
    "                    special_characters=True, feature_names = feature_cols,class_names=['0','1'])\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "    graph.write_png('magic_tree.png')\n",
    "    Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    #classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcMetrics(Yt, Yp, classes, model, norm=True, cols=None, print_metrics=False):\n",
    "    \n",
    "    if(len(classes) == 2):\n",
    "        average = \"binary\"\n",
    "    else:\n",
    "        average = None\n",
    "    \n",
    "    # Metrics\n",
    "    acc = metrics.accuracy_score(Yt, Yp)\n",
    "    acc_bal = metrics.balanced_accuracy_score(Yt, Yp)\n",
    "    prec = metrics.precision_score(Yt, Yp, average=average)\n",
    "    recall = metrics.recall_score(Yt, Yp, average=average)\n",
    "    f1 = metrics.f1_score(Yt, Yp, average=average)\n",
    "    #roc_auc = roc_auc_score(Yt, Yp, average=average)\n",
    "    roc_auc = 0.0\n",
    "    \n",
    "    feature_imp = pd.Series(model.feature_importances_,index=cols).sort_values(ascending=True)\n",
    "    \n",
    "    if(print_metrics):\n",
    "        print(\"\")\n",
    "        plot_confusion_matrix(Yt, Yp, classes, normalize=norm)\n",
    "        print(\"\")\n",
    "        print(\"Accuracy:\", acc)\n",
    "        print(\"Balanced Accuracy: \", acc_bal)\n",
    "        print(\"Precision: \", prec)\n",
    "        print(\"Recall: \", recall)\n",
    "        print(\"F1 Score: \", f1)\n",
    "        print(\"ROC AuC Score: \", roc_auc)\n",
    "       \n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "        ax.barh(feature_imp.index,feature_imp.values, align='center')\n",
    "        ax.set_xlabel('Performance')\n",
    "        ax.set_title('How fast do you want to go today?')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    return acc, acc_bal, prec, recall, f1, roc_auc, feature_imp\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decisionTreeFactory(depth=4, criterion='gini', weights=None, classes=[0,1]):\n",
    "    \n",
    "    def trainDecisionTree(Xtr, Xte, Ytr, Yte, feat_cols, print_metrics=False):\n",
    "        model = DecisionTreeClassifier(max_depth=depth, criterion=criterion, class_weight=weights)\n",
    "        model = model.fit(Xtr,Ytr)\n",
    "        y_pred = model.predict(Xte)    \n",
    "        return calcMetrics(Yte, y_pred, classes, model, norm=True, cols=feat_cols, \n",
    "                           print_metrics=print_metrics), model\n",
    "    \n",
    "    return trainDecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomForestFactory(n_estimators=1000, depth=None, weights=None, classes=[0,1]):\n",
    "    \n",
    "    def trainRandomForest(Xtr, Xte, Ytr, Yte, feat_cols, print_metrics=False):\n",
    "        model=RandomForestClassifier(n_estimators=n_estimators, class_weight=weights, max_depth=depth)\n",
    "        model.fit(Xtr,Ytr)    \n",
    "        y_pred=model.predict(Xte)\n",
    "\n",
    "        return calcMetrics(Yte, y_pred, classes, model, norm=True, cols=feat_cols,\n",
    "                           print_metrics=print_metrics), model\n",
    "    \n",
    "    return trainRandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ADAfactory(estimator=None, n_estim=1000, classes=[0,1]):\n",
    "\n",
    "    def trainADA(Xtr, Xte, Ytr, Yte, feat_cols, print_metrics=False):\n",
    "        model = AdaBoostClassifier(base_estimator=estimator, n_estimators=n_estim)\n",
    "        model.fit(Xtr , Ytr)\n",
    "        y_pred = model.predict(Xte)\n",
    "\n",
    "        return calcMetrics(Yte, y_pred, classes, model, norm=True,\n",
    "                           cols=feat_cols, print_metrics=print_metrics), model\n",
    "    \n",
    "    return trainADA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientBoostingFactory(n_estim=1000, classes=[0,1]):\n",
    "    \n",
    "    def trainGradientBoosting(Xtr, Xte, Ytr, Yte, feat_cols, print_metrics=False):\n",
    "        model = GradientBoostingClassifier(n_estimators=n_estim)\n",
    "        model.fit(Xtr , Ytr)\n",
    "        y_pred = model.predict(Xte)\n",
    "\n",
    "        return calcMetrics(Yte, y_pred, classes, model, norm=True,\n",
    "                           cols=feat_cols, print_metrics=print_metrics), model\n",
    "    \n",
    "    return trainGradientBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainDNN(Xtr, Xte, Xv, Ytr, Yte, Yv, feat_cols, print_metrics=False):\n",
    "    model = models.Sequential()\n",
    "    model.add(Dense(13,input_dim = 31, activation='relu'))\n",
    "    model.add(Dense(13, activation='relu'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='softmax'))\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer = \"rmsprop\", metrics = ['accuracy'])\n",
    "\n",
    "    history = model.fit(Xtr, Ytr,\n",
    "                       epochs=7,\n",
    "                       batch_size=20,\n",
    "                       validation_data=(Xv, Yv))\n",
    "    \n",
    "    y_pred = model.predict(Xte)\n",
    "    print(y_pred)\n",
    "\n",
    "    return classification_report(Yte, y_pred)\n",
    "\n",
    "    #h_dict = history.history\n",
    "    #loss_values = h_dict['loss']\n",
    "    #valid_loss_values = h_dict['val_loss']\n",
    "    #acc_values = h_dict['acc']\n",
    "    #valid_acc_values = h_dict['val_acc']\n",
    "    #epochs = range(1, len(loss_values) +1)\n",
    "    #fig, axes = plt.subplots(2, figsize=(10,10))\n",
    "    #axes[0].plot(epochs, loss_values, 'bo', label='Training loss')\n",
    "    #axes[0].plot(epochs, valid_loss_values, 'b', label='Validation loss')\n",
    "    #axes[1].plot(epochs, acc_values, 'bo', label='Training Accuracy')\n",
    "    #axes[1].plot(epochs, valid_acc_values, 'b', label='Validation Accuracy')\n",
    "    #plt.legend()\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelMetricAggregator:\n",
    "    \n",
    "    def __init__(self, model, mode=''):\n",
    "        self.model = model\n",
    "        self.mode = mode\n",
    "        self.accuracy = []\n",
    "        self.balanced_accuracy = []\n",
    "        self.precision = []\n",
    "        self.recall = []\n",
    "        self.f1_score = []\n",
    "        self.roc_auc = []\n",
    "        \n",
    "    def addMetrics(self,acc, bal_acc, prec, rec, f1, roc_auc):\n",
    "        self.accuracy.append(acc)\n",
    "        self.balanced_accuracy.append(bal_acc)\n",
    "        self.precision.append(prec)\n",
    "        self.recall.append(rec)\n",
    "        self.f1_score.append(f1)\n",
    "        self.roc_auc.append(roc_auc)\n",
    "        \n",
    "    def getMetrics(self):\n",
    "        return [\n",
    "            self.model,\n",
    "            self.mode,\n",
    "            np.mean(self.accuracy),\n",
    "            np.mean(self.balanced_accuracy),\n",
    "            np.mean(self.precision),\n",
    "            np.mean(self.recall),\n",
    "            np.mean(self.f1_score),\n",
    "            np.mean(self.roc_auc)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossValSMOTE(X, y, cols, model_names, train_eval_methods, split_mode='stratified'):\n",
    "\n",
    "    metrics_cols = ['baseline','model', 'accuracy', 'bal_accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']\n",
    "    mDF = pd.DataFrame(columns=metrics_cols)\n",
    "    \n",
    "    if(len(model_names) != len(train_eval_methods)):\n",
    "        return\n",
    "    \n",
    "    splitter = StratifiedKFold(n_splits=4)\n",
    "    split_gen = splitter.split(X, y)\n",
    "    \n",
    "    if(split_mode == 'kfold'):\n",
    "        splitter = KFold(n_splits=4, random_state=11)\n",
    "        split_gen = splitter.split(X)\n",
    "    \n",
    "    aggs = []\n",
    "    \n",
    "    for i in range(0, len(model_names)):\n",
    "        aggregator = ModelMetricAggregator(\"sub\", model_names[i])\n",
    "        aggs.append(aggregator)\n",
    "        \n",
    "    \n",
    "    for train_index, test_index in split_gen:\n",
    "        # Generate Train and Test set\n",
    "        X_train = X.iloc[train_index]        \n",
    "        y_train = y.iloc[train_index]\n",
    "        X_test = X.iloc[test_index]\n",
    "        y_test = y.iloc[test_index]\n",
    "    \n",
    "        # SMOTE inside the fold\n",
    "        X_train, y_train = SMOTE().fit_resample(X_train, y_train)\n",
    "        \n",
    "        for i in range(0, len(model_names)):\n",
    "            (acc, bal_acc, prec, rec, f1, auc, best), model = \\\n",
    "                train_eval_methods[i](X_train, X_test, y_train, y_test, cols)\n",
    "            \n",
    "            aggs[i].addMetrics(acc, bal_acc, prec, rec, f1, auc)            \n",
    "     \n",
    "    \n",
    "    for agg in aggs:\n",
    "        row = agg.getMetrics()\n",
    "        series = pd.Series(row, index=metrics_cols)\n",
    "        mDF = mDF.append(series, ignore_index=True)\n",
    "    \n",
    "    return mDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 \ttree \tsub \t0.637019 \t0.524242 \t0.199107 \t0.354167 \t0.245040 \t0.35\n",
    "\n",
    "# 3 \tada \tsub \t0.763636 \t0.616389 \t0.300000 \t0.40 \t0.320000 \t0.6250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_files = [ #sub is better, from the literature\n",
    "    \"features/features_sub.csv\",\n",
    "    #\"features/features_div.csv\"\n",
    "]\n",
    "\n",
    "feature_cols = [\n",
    "        #'duration',\n",
    "        #'show_order',\n",
    "        'card_class',\n",
    "        'fix_freq','sacc_freq',\n",
    "        'pupil_diam_right_mean',\n",
    "        'pupil_diam_right_std',\n",
    "        'pupil_diam_right_min',\n",
    "        'pupil_diam_right_max',\n",
    "        'pupil_diam_left_mean',\n",
    "        'pupil_diam_left_std',\n",
    "        'pupil_diam_left_min',\n",
    "        'pupil_diam_left_max',\n",
    "    \n",
    "        'sre_fix_freq','sre_sacc_freq',\n",
    "        'sre_pupil_diam_right_mean',\n",
    "        'sre_pupil_diam_right_std',\n",
    "        'sre_pupil_diam_right_min',\n",
    "        'sre_pupil_diam_right_max',\n",
    "        'sre_pupil_diam_left_mean',\n",
    "        'sre_pupil_diam_left_std',\n",
    "        'sre_pupil_diam_left_min',\n",
    "        'sre_pupil_diam_left_max',\n",
    "    \n",
    "        'srl_fix_freq','srl_sacc_freq',\n",
    "        'srl_pupil_diam_right_mean',\n",
    "        'srl_pupil_diam_right_std',\n",
    "        'srl_pupil_diam_right_min',\n",
    "        'srl_pupil_diam_right_max',\n",
    "        'srl_pupil_diam_left_mean',\n",
    "        'srl_pupil_diam_left_std',\n",
    "        'srl_pupil_diam_left_min',\n",
    "        'srl_pupil_diam_left_max',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeWeights(Ytr):\n",
    "    class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(Ytr),\n",
    "                                                 Ytr)\n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ VALIDATION =============\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>baseline</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>bal_accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sub</td>\n",
       "      <td>tree</td>\n",
       "      <td>0.558148</td>\n",
       "      <td>0.531358</td>\n",
       "      <td>0.182765</td>\n",
       "      <td>0.4875</td>\n",
       "      <td>0.262522</td>\n",
       "      <td>0.531358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sub</td>\n",
       "      <td>forest</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.543128</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.255357</td>\n",
       "      <td>0.543128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sub</td>\n",
       "      <td>ada</td>\n",
       "      <td>0.723704</td>\n",
       "      <td>0.580330</td>\n",
       "      <td>0.305556</td>\n",
       "      <td>0.3625</td>\n",
       "      <td>0.294185</td>\n",
       "      <td>0.580330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sub</td>\n",
       "      <td>gb</td>\n",
       "      <td>0.704444</td>\n",
       "      <td>0.524080</td>\n",
       "      <td>0.170455</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.188095</td>\n",
       "      <td>0.524080</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  baseline   model  accuracy  bal_accuracy  precision  recall  f1_score  \\\n",
       "0      sub    tree  0.558148      0.531358   0.182765  0.4875  0.262522   \n",
       "1      sub  forest  0.703704      0.543128   0.291667  0.3000  0.255357   \n",
       "2      sub     ada  0.723704      0.580330   0.305556  0.3625  0.294185   \n",
       "3      sub      gb  0.704444      0.524080   0.170455  0.2500  0.188095   \n",
       "\n",
       "    roc_auc  \n",
       "0  0.531358  \n",
       "1  0.543128  \n",
       "2  0.580330  \n",
       "3  0.524080  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = \"features/features_none.csv\"\n",
    "file_test = \"features/PILOT/features_none.csv\"\n",
    "f = file.split('/')[-1]\n",
    "mode = file.split('/')[-1].split('.')[0].split('_')\n",
    "    \n",
    "# Load the file\n",
    "data = pd.read_csv(file, sep='\\t')\n",
    "test = pd.read_csv(file_test, sep='\\t')\n",
    "    \n",
    "if(mode[1] == \"sub\"):\n",
    "    data = data.fillna(0)\n",
    "    test = test.fillna(0)\n",
    "else:\n",
    "    data = data.fillna(1)\n",
    "    test = test.fillna(1)\n",
    "    \n",
    "# Extract Feature Columns\n",
    "X = data[feature_cols]\n",
    "y = data['label']\n",
    "\n",
    "w_list = computeWeights(y)\n",
    "w_train = {\n",
    "    0:w_list[0],\n",
    "    1:w_list[1]\n",
    "}\n",
    "\n",
    "X_test = test[feature_cols]\n",
    "y_test = test['label']\n",
    "\n",
    "print(\"============ VALIDATION =============\")\n",
    "\n",
    "valDF = crossValSMOTE(X, y,\n",
    "                      feature_cols,\n",
    "                      ['tree', 'forest', 'ada', 'gb'],\n",
    "                      [decisionTreeFactory(),\n",
    "                       randomForestFactory(),\n",
    "                       ADAfactory(),\n",
    "                       gradientBoostingFactory()\n",
    "                      ],\n",
    "                      split_mode='stratified')\n",
    "valDF.head()\n",
    "\n",
    "#print(\"============ TEST =============\")\n",
    "# SMOTE X y\n",
    "#X, y = SMOTE().fit_resample(X, y)\n",
    "#e_metr, model = trainRandomForest(X, X_test, y, y_test, feature_cols, print_metrics=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ VALIDATION =============\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_12 to have shape (6,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-ec3657122856>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     76\u001b[0m                        \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                        \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                        validation_data=(X_val, y_val))\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;31m#res = model.evaluate(X_test, y_test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_12 to have shape (6,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "card_features_col = [\n",
    "        #'duration',\n",
    "        'fix_freq','sacc_freq',\n",
    "        #'pupil_diam_right_mean',\n",
    "        'pupil_diam_right_std',\n",
    "        'pupil_diam_right_min',\n",
    "        #'pupil_diam_right_max',\n",
    "        #'pupil_diam_left_mean',\n",
    "        'pupil_diam_left_std',\n",
    "        'pupil_diam_left_min',\n",
    "        #'pupil_diam_left_max',\n",
    "    \n",
    "        #'sre_fix_freq','sre_sacc_freq',\n",
    "        #'sre_pupil_diam_right_mean',\n",
    "        #'sre_pupil_diam_right_std',\n",
    "        #'sre_pupil_diam_right_min',\n",
    "        #'sre_pupil_diam_right_max',\n",
    "        #'sre_pupil_diam_left_mean',\n",
    "        #'sre_pupil_diam_left_std',\n",
    "        #'sre_pupil_diam_left_min',\n",
    "        #'sre_pupil_diam_left_max',\n",
    "    \n",
    "        #'srl_fix_freq','srl_sacc_freq',\n",
    "        #'srl_pupil_diam_right_mean',\n",
    "        #'srl_pupil_diam_right_std',\n",
    "        #'srl_pupil_diam_right_min',\n",
    "        #'srl_pupil_diam_right_max',\n",
    "        #'srl_pupil_diam_left_mean',\n",
    "        #'srl_pupil_diam_left_std',\n",
    "        #'srl_pupil_diam_left_min',\n",
    "        #'srl_pupil_diam_left_max',\n",
    "]\n",
    "\n",
    "file = \"features/features_sub.csv\"\n",
    "file_test = \"features/PILOT/features_sub.csv\"\n",
    "\n",
    "# Load the file\n",
    "data = pd.read_csv(file, sep='\\t')\n",
    "test = pd.read_csv(file_test, sep='\\t')\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(data['card_class'])\n",
    "data['card_class_enc'] = encoder.transform(data['card_class'])\n",
    "test['card_class_enc'] = encoder.transform(test['card_class'])\n",
    "    \n",
    "if(mode[1] == \"sub\"):\n",
    "    data = data.fillna(0)\n",
    "    test = test.fillna(0)\n",
    "else:\n",
    "    data = data.fillna(1)\n",
    "    test = test.fillna(1)\n",
    "    \n",
    "# Extract Feature Columns\n",
    "X = data[card_features_col]\n",
    "y = data['card_class']\n",
    "\n",
    "X_test = test[card_features_col]\n",
    "y_test = test['card_class']\n",
    "\n",
    "#classes = ['unicorn', 'hedge', 'pepper', 'aliens', 'minion', 'pig']\n",
    "classes = [1,2,3,4,5,6]\n",
    "\n",
    "print(\"============ VALIDATION =============\")\n",
    "\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "#randomForestFactory(classes=classes)(X_train, X_test, y_train, y_test, card_features_col, print_metrics=True)\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(Dense(8,input_dim = len(card_features_col), activation='relu'))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer = \"rmsprop\", metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                       epochs=20,\n",
    "                       batch_size=10,\n",
    "                       validation_data=(X_val, y_val))\n",
    "    \n",
    "#res = model.evaluate(X_test, y_test)\n",
    "\n",
    "h_dict = history.history\n",
    "loss_values = h_dict['loss']\n",
    "valid_loss_values = h_dict['val_loss']\n",
    "acc_values = h_dict['acc']\n",
    "\n",
    "valid_acc_values = h_dict['val_acc']\n",
    "epochs = range(1, len(loss_values) +1)\n",
    "fig, axes = plt.subplots(2, figsize=(10,10))\n",
    "\n",
    "axes[0].plot(epochs, loss_values, 'bo', label='Training loss')\n",
    "axes[0].plot(epochs, valid_loss_values, 'b', label='Validation loss')\n",
    "axes[1].plot(epochs, acc_values, 'bo', label='Training Accuracy')\n",
    "axes[1].plot(epochs, valid_acc_values, 'b', label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
